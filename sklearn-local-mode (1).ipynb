{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "859de4ac",
   "metadata": {},
   "source": [
    "## SageMaker Local Mode Sklearn Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd4998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import subprocess\n",
    "\n",
    "#Setup\n",
    "boto_session = boto3.session.Session()\n",
    "s3 = boto_session.resource('s3')\n",
    "region = boto_session.region_name\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8ac07",
   "metadata": {},
   "source": [
    "## Local Mode Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82b34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting local_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile local_model.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "\n",
    "#Load data\n",
    "boston = datasets.load_boston()\n",
    "df = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "df['MEDV'] = boston.target \n",
    "\n",
    "#Split Model\n",
    "X = df.drop(['MEDV'], axis = 1) \n",
    "y = df['MEDV']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42)\n",
    "\n",
    "#Model Creation\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "with open('model.joblib', 'wb') as f:\n",
    "    joblib.dump(lm,f)\n",
    "\n",
    "\n",
    "with open('model.joblib', 'rb') as f:\n",
    "    predictor = joblib.load(f)\n",
    "\n",
    "print(\"Testing following input: \")\n",
    "print(X_test[0:1])\n",
    "sampInput = [[0.09178, 0.0, 4.05, 0.0, 0.51, 6.416, 84.1, 2.6463, 5.0, 296.0, 16.6, 395.5, 9.04]]\n",
    "print(type(sampInput))\n",
    "print(predictor.predict(sampInput))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a86a3",
   "metadata": {},
   "source": [
    "### Local Mode Training\n",
    "\n",
    "Run local_model.py to generate a model.joblib (model data) and tar this with your inference.py script to create a model.tar.gz for your SageMaker Local Endpoint to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58aeab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Testing following input: \n",
      "        CRIM   ZN  INDUS  CHAS   NOX  ...  RAD    TAX  PTRATIO      B  LSTAT\n",
      "173  0.09178  0.0   4.05   0.0  0.51  ...  5.0  296.0     16.6  395.5   9.04\n",
      "\n",
      "[1 rows x 13 columns]\n",
      "<class 'list'>\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/sklearn/base.py:445: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "[28.99672362]\n"
     ]
    }
   ],
   "source": [
    "!python local_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c02eac",
   "metadata": {},
   "source": [
    "### Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0cc5395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "Deserialize fitted model\n",
    "\"\"\"\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "input_fn\n",
    "    request_body: The body of the request sent to the model.\n",
    "    request_content_type: (string) specifies the format/variable type of the request\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    print(\"-----------------------\")\n",
    "    print(request_body)\n",
    "    print(request_content_type)\n",
    "    print(\"-----------------------\")\n",
    "    if request_content_type == 'application/json':\n",
    "        request_body = json.loads(request_body)\n",
    "        inpVar = request_body['Input']\n",
    "        return inpVar\n",
    "    else:\n",
    "        raise ValueError(\"This model only supports application/json input\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "predict_fn\n",
    "    input_data: returned array from input_fn above\n",
    "    model (sklearn model) returned model loaded from model_fn above\n",
    "\"\"\"\n",
    "def predict_fn(input_data, model):\n",
    "    print(\"-----------------------\")\n",
    "    print(input_data)\n",
    "    print(type(input_data))\n",
    "    print(\"-----------------------\")\n",
    "    return model.predict(input_data)\n",
    "\n",
    "\"\"\"\n",
    "output_fn\n",
    "    prediction: the returned value from predict_fn above\n",
    "    content_type: the content type the endpoint expects to be returned. Ex: JSON, string\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    res = int(prediction[0])\n",
    "    respJSON = {'Output': res}\n",
    "    print(\"-----------------------\")\n",
    "    print(respJSON)\n",
    "    print(\"-----------------------\")\n",
    "    return respJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6eddf",
   "metadata": {},
   "source": [
    "### Create model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5cffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build tar file with model data + inference code\n",
    "bashCommand = \"tar -cvpzf model.tar.gz model.joblib\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f2c9f",
   "metadata": {},
   "source": [
    "## Push Model Data to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c025525",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"local-mode-mars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe9f066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: local-mode-mars\n"
     ]
    }
   ],
   "source": [
    "!aws s3 mb \"s3://{bucket_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26121b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload tar.gz to bucket\n",
    "model_artifacts = f\"s3://{bucket_name}/model.tar.gz\"\n",
    "response = s3.meta.client.upload_file('model.tar.gz', bucket_name, 'model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7bdf2",
   "metadata": {},
   "source": [
    "## Enable Local Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e2d5935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.sklearn import SKLearn, SKLearnModel\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40d0b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = LocalSession()\n",
    "session.config = {'local': {'local_code': True}}\n",
    "model = SKLearnModel(\n",
    "    entry_point='inference.py',\n",
    "    role=role,\n",
    "    model_data=model_artifacts,\n",
    "    framework_version='0.23-1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9b29633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sagemaker.local.local_session.LocalSession'>\n"
     ]
    }
   ],
   "source": [
    "print(type(session)) #verify local session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd069f",
   "metadata": {},
   "source": [
    "### Sample Payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a06df12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.1780e-02, 0.0000e+00, 4.0500e+00, 0.0000e+00, 5.1000e-01,\n",
       "        6.4160e+00, 8.4100e+01, 2.6463e+00, 5.0000e+00, 2.9600e+02,\n",
       "        1.6600e+01, 3.9550e+02, 9.0400e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "payload = np.array([[0.09178, 0.0, 4.05, 0.0, 0.51, 6.416, 84.1, 2.6463, 5.0, 296.0, 16.6, 395.5, 9.04]])\n",
    "payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db09fd",
   "metadata": {},
   "source": [
    "## Sample Deployment & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f865d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to 8pidzry78j-algo-1-es9ue\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m 2022-08-08 17:29:42,165 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m 2022-08-08 17:29:42,167 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m 2022-08-08 17:29:42,168 INFO - sagemaker-containers - nginx config: \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m worker_processes auto;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m daemon off;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m pid /tmp/nginx.pid;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m error_log  /dev/stderr;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m worker_rlimit_nofile 4096;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m events {\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   worker_connections 2048;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m }\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m http {\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   include /etc/nginx/mime.types;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   default_type application/octet-stream;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   access_log /dev/stdout combined;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   upstream gunicorn {\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m     server unix:/tmp/gunicorn.sock;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   }\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   server {\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m     listen 8080 deferred;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m     client_max_body_size 0;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m     keepalive_timeout 3;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m     location ~ ^/(ping|invocations|execution-parameters) {\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m       proxy_set_header Host $http_host;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m       proxy_redirect off;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m       proxy_read_timeout 60s;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m       proxy_pass http://gunicorn;\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m     }\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m     location / {\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m       return 404 \"{}\";\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m     }\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   }\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m }\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m 2022-08-08 17:29:42,317 INFO - sagemaker-containers - Module inference does not provide a setup.py. \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m Generating setup.py\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m 2022-08-08 17:29:42,318 INFO - sagemaker-containers - Generating setup.cfg\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m 2022-08-08 17:29:42,318 INFO - sagemaker-containers - Generating MANIFEST.in\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m 2022-08-08 17:29:42,318 INFO - sagemaker-containers - Installing module with the following command:\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m /miniconda3/bin/python3 -m pip install . \n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \u001b[?25hBuilding wheels for collected packages: inference\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   Building wheel for inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \u001b[?25h  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3808 sha256=5114fd75399c57f1e7477d85c3f692e439b2c49974ec012866bfe13fffdc7a2b\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-a77d51n9/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m Successfully built inference\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m Installing collected packages: inference\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m Successfully installed inference-1.0.0\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m You should consider upgrading via the '/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m \u001b[0m[2022-08-08 17:29:43 +0000] [34] [INFO] Starting gunicorn 20.0.4\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m [2022-08-08 17:29:43 +0000] [34] [INFO] Listening at: unix:/tmp/gunicorn.sock (34)\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m [2022-08-08 17:29:43 +0000] [34] [INFO] Using worker: gevent\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m [2022-08-08 17:29:43 +0000] [37] [INFO] Booting worker with pid: 37\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m [2022-08-08 17:29:43 +0000] [38] [INFO] Booting worker with pid: 38\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m [2022-08-08 17:29:43 +0000] [39] [INFO] Booting worker with pid: 39\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m [2022-08-08 17:29:43 +0000] [43] [INFO] Booting worker with pid: 43\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m 2022-08-08 17:29:45,808 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m /miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LinearRegression from version 1.0.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   UserWarning)\n",
      "!\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m 172.18.0.1 - - [08/Aug/2022:17:29:46 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"python-urllib3/1.26.8\"\n",
      "<sagemaker.sklearn.model.SKLearnPredictor object at 0x7f97fc6ea550>\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m 2022-08-08 17:29:46,162 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m /miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LinearRegression from version 1.0.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   UserWarning)\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m -----------------------\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m [[9.1780e-02 0.0000e+00 4.0500e+00 0.0000e+00 5.1000e-01 6.4160e+00\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   8.4100e+01 2.6463e+00 5.0000e+00 2.9600e+02 1.6600e+01 3.9550e+02\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m   9.0400e+00]]\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m <class 'numpy.ndarray'>\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m -----------------------\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m -----------------------\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m {'Output': 28}\n",
      "\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m -----------------------\n",
      "Failed to interpret file <_io.BytesIO object at 0x7f97fc649a90> as a pickle\u001b[36m8pidzry78j-algo-1-es9ue |\u001b[0m 172.18.0.1 - - [08/Aug/2022:17:29:46 +0000] \"POST /invocations HTTP/1.1\" 200 14 \"-\" \"python-urllib3/1.26.8\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    predictor = model.deploy(initial_instance_count=1, instance_type='local')\n",
    "    print(predictor)\n",
    "    preds = predictor.predict(payload)\n",
    "    print(preds)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
